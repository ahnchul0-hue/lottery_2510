# ğŸ” Pattern Analyzer Agent - Neural Pattern Recognition Specialist
# Advanced pattern recognition using PyTorch Transformers

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import json
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import logging

from .base import BaseAgent, AgentCapabilities, AgentStatus
from ..models.lottery_transformer import LotteryPatternPredictor
from ..services.data_loader import get_data_loader

class PositionalEncoding(nn.Module):
    """Transformerìš© ìœ„ì¹˜ ì¸ì½”ë”©"""
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class LotteryTransformer(nn.Module):
    """ë¡œë˜ ë²ˆí˜¸ íŒ¨í„´ í•™ìŠµìš© Transformer ëª¨ë¸"""
    def __init__(self, 
                 d_model: int = 128,
                 n_heads: int = 8,
                 n_layers: int = 4,
                 dim_feedforward: int = 512,
                 max_seq_len: int = 100,
                 vocab_size: int = 46):  # 1-45 + padding
        super().__init__()
        
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # ë²ˆí˜¸ ì„ë² ë”© (1-45 ë²ˆí˜¸ â†’ d_model ì°¨ì›)
        self.number_embedding = nn.Embedding(vocab_size, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # Transformer ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        
        # ì¶œë ¥ í—¤ë“œë“¤
        self.probability_head = nn.Linear(d_model, 45)  # ë²ˆí˜¸ë³„ í™•ë¥ 
        self.pattern_head = nn.Linear(d_model, 64)      # íŒ¨í„´ íŠ¹ì„±
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        embedded = self.number_embedding(x) * math.sqrt(self.d_model)
        embedded = self.positional_encoding(embedded)
        embedded = self.dropout(embedded)
        
        # Transformer ì¸ì½”ë”©
        transformer_out = self.transformer(embedded)
        
        # ë§ˆì§€ë§‰ í† í°ì˜ ì¶œë ¥ ì‚¬ìš©
        last_output = transformer_out[:, -1, :]
        
        # ê° í—¤ë“œë³„ ì¶œë ¥
        probabilities = F.softmax(self.probability_head(last_output), dim=-1)
        pattern_features = torch.tanh(self.pattern_head(last_output))
        
        return {
            'probabilities': probabilities,
            'pattern_features': pattern_features,
            'hidden_states': transformer_out
        }

class PatternAnalyzerAgent(BaseAgent):
    """
    íŒ¨í„´ ë¶„ì„ ì—ì´ì „íŠ¸
    
    PyTorch Transformerë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ íŒ¨í„´ ì¸ì‹:
    - ì‹œê³„ì—´ ë²ˆí˜¸ ì‹œí€€ìŠ¤ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ ë°œê²¬
    - Multi-head Attentionìœ¼ë¡œ ë²ˆí˜¸ê°„ ìƒê´€ê´€ê³„ í¬ì°©
    - í™•ë¥  ë¶„í¬ ê¸°ë°˜ ì •êµí•œ ì˜ˆì¸¡
    - ì°½ì˜ì  ë²ˆí˜¸ ì¡°í•© ìƒì„±
    """
    
    def __init__(self, agent_id: str, memory_manager, message_bus, config: Dict[str, Any]):
        super().__init__(agent_id, memory_manager, message_bus, config)
        
        self.capabilities = [
            AgentCapabilities.PATTERN_RECOGNITION,
            AgentCapabilities.NEURAL_PROCESSING
        ]
        
        # ëª¨ë¸ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model: Optional[LotteryPatternPredictor] = None
        self.model_config = {
            'd_model': config.get('d_model', 64),
            'nhead': config.get('n_heads', 8),
            'num_layers': config.get('n_layers', 4),
            'dropout': config.get('dropout', 0.1)
        }
        
        # íŒ¨í„´ ë¶„ì„ ì„¤ì •
        self.sequence_length = config.get('sequence_length', 20)
        self.prediction_diversity = config.get('prediction_diversity', 0.3)
        
        # íŒ¨í„´ ë©”ëª¨ë¦¬
        self.discovered_patterns = {}
        self.pattern_confidence = {}
        
    async def initialize(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        self.logger.info(f"ğŸ” Initializing Pattern Analyzer Agent {self.agent_id}")
        
        try:
            # ìƒˆë¡œìš´ LotteryPatternPredictor ëª¨ë¸ ì´ˆê¸°í™”
            self.model = LotteryPatternPredictor(self.model_config)
            success = self.model.initialize()
            
            if not success:
                self.logger.warning("âš ï¸ Model initialization failed, using fallback")
            
            # ë°ì´í„° ë¡œë” ì—°ê²°
            self.data_loader = await get_data_loader()
            
            # ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
            await self._analyze_base_patterns()
            
            self.status = AgentStatus.ACTIVE
            self.logger.info("âœ… Pattern Analyzer Agent initialized")
            
        except Exception as e:
            self.status = AgentStatus.ERROR
            self.logger.error(f"âŒ Failed to initialize Pattern Analyzer: {e}")
            raise
            
    async def process_prediction(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ ì²˜ë¦¬"""
        machine_type = task_data['machine_type']
        sets_count = task_data['sets_count']
        
        self.logger.info(f"ğŸ§  Processing pattern analysis for {machine_type}")
        
        try:
            # 1. ê³¼ê±° ë°ì´í„° ë¡œë“œ (ìƒˆë¡œìš´ ë°ì´í„° ë¡œë” ì‚¬ìš©)
            historical_data = self._get_historical_numbers(machine_type)
            
            # 2. ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ ìƒì„±
            if self.model and hasattr(self.model, 'predict'):
                model_predictions = self.model.predict(
                    historical_data, 
                    sets_count,
                    diversity_threshold=self.prediction_diversity
                )
                
                # ê²°ê³¼ í¬ë§· ì¡°ì •
                predictions = []
                for i, pred in enumerate(model_predictions):
                    pattern_result = {
                        'set_number': i + 1,
                        'numbers': pred['numbers'],
                        'confidence_score': pred['confidence'],
                        'pattern_analysis': self._analyze_pattern_for_numbers(pred['numbers'], machine_type),
                        'selection_method': pred.get('method', 'transformer_pattern'),
                        'diversity_factor': self.prediction_diversity,
                        'metadata': pred.get('metadata', {})
                    }
                    predictions.append(pattern_result)
            else:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ fallback ì‚¬ìš©
                predictions = await self._fallback_predictions(historical_data, sets_count, machine_type)
                
            # 3. ë©”íƒ€ ì •ë³´ ìƒì„±
            metadata = await self._generate_metadata(machine_type, predictions)
            
            return {
                'success': True,
                'agent_id': self.agent_id,
                'agent_type': 'pattern_analyzer',
                'predictions': predictions,
                'metadata': metadata,
                'avg_confidence': np.mean([p['confidence_score'] for p in predictions])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Pattern prediction failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'agent_id': self.agent_id,
                'agent_type': 'pattern_analyzer'
            }
            
    async def get_specialization_info(self) -> Dict[str, Any]:
        """íŠ¹í™” ì •ë³´ ë°˜í™˜"""
        return {
            'specialization': 'Deep Learning Pattern Recognition',
            'model_architecture': 'PyTorch Transformer',
            'capabilities': [
                'Sequential pattern analysis',
                'Multi-head attention correlation',
                'Probability distribution prediction',
                'Creative number combination generation'
            ],
            'model_config': self.model_config,
            'discovered_patterns': len(self.discovered_patterns),
            'device': str(self.device)
        }
        
    async def _load_historical_data(self, machine_type: str, limit: int = 50) -> List[Dict[str, Any]]:
        """ê³¼ê±° ë°ì´í„° ë¡œë“œ"""
        return await self.memory_manager.get_lottery_data(
            machine_type=machine_type,
            limit=limit
        )
        
    def _prepare_sequences(self, historical_data: List[Dict[str, Any]]) -> torch.Tensor:
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„"""
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
        sorted_data = sorted(historical_data, key=lambda x: x['íšŒì°¨'])
        
        # ë²ˆí˜¸ ì‹œí€€ìŠ¤ ìƒì„±
        sequences = []
        for record in sorted_data:
            numbers = record['1ë“±_ë‹¹ì²¨ë²ˆí˜¸']
            # íŒ¨ë”© í† í° 0ìœ¼ë¡œ ì‹œì‘, ë²ˆí˜¸ëŠ” 1-45
            sequence = [0] + numbers  # [0, num1, num2, num3, num4, num5, num6]
            sequences.append(sequence)
            
        # Tensorë¡œ ë³€í™˜
        max_len = max(len(seq) for seq in sequences)
        padded_sequences = []
        
        for seq in sequences:
            if len(seq) < max_len:
                seq.extend([0] * (max_len - len(seq)))
            padded_sequences.append(seq)
            
        return torch.tensor(padded_sequences, dtype=torch.long).to(self.device)
        
    async def _predict_with_pattern_analysis(self, 
                                           sequences: torch.Tensor,
                                           machine_type: str,
                                           prediction_index: int) -> Dict[str, Any]:
        """íŒ¨í„´ ë¶„ì„ ê¸°ë°˜ ì˜ˆì¸¡"""
        
        with torch.no_grad():
            # ëª¨ë¸ ì¶”ë¡ 
            outputs = self.model(sequences[-self.sequence_length:])
            
            probabilities = outputs['probabilities'][0]  # ë°°ì¹˜ ì°¨ì› ì œê±°
            pattern_features = outputs['pattern_features'][0]
            
            # ìƒìœ„ í™•ë¥  ë²ˆí˜¸ë“¤ ì„ íƒ (ë‹¤ì–‘ì„± ê³ ë ¤)
            selected_numbers = self._select_numbers_with_diversity(
                probabilities.cpu().numpy(),
                diversity_factor=self.prediction_diversity + (prediction_index * 0.1)
            )
            
            # íŒ¨í„´ íŠ¹ì„± ë¶„ì„
            pattern_analysis = self._analyze_pattern_features(
                pattern_features.cpu().numpy(),
                machine_type
            )
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_pattern_confidence(
                selected_numbers, probabilities.cpu().numpy(), pattern_analysis
            )
            
            return {
                'set_number': prediction_index + 1,
                'numbers': sorted(selected_numbers),
                'confidence_score': confidence_score,
                'pattern_analysis': pattern_analysis,
                'selection_method': 'transformer_attention',
                'diversity_factor': self.prediction_diversity + (prediction_index * 0.1)
            }
            
    def _select_numbers_with_diversity(self, 
                                     probabilities: np.ndarray,
                                     diversity_factor: float = 0.3) -> List[int]:
        """ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ë²ˆí˜¸ ì„ íƒ"""
        
        # ìƒìœ„ í™•ë¥  ë²ˆí˜¸ë“¤ (1-45 ë²”ìœ„)
        top_indices = np.argsort(probabilities)[-15:]  # ìƒìœ„ 15ê°œ
        top_probs = probabilities[top_indices]
        
        selected = []
        
        # ì²« ë²ˆì§¸ëŠ” ìµœê³  í™•ë¥  ë²ˆí˜¸
        best_idx = top_indices[np.argmax(top_probs)]
        selected.append(best_idx + 1)  # 1-45 ë²”ìœ„ë¡œ ë³€í™˜
        
        # ë‚˜ë¨¸ì§€ 5ê°œëŠ” ë‹¤ì–‘ì„± ê³ ë ¤í•˜ì—¬ ì„ íƒ
        remaining_indices = [idx for idx in top_indices if idx != best_idx]
        
        while len(selected) < 6 and remaining_indices:
            # í™•ë¥ ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•
            scores = []
            for idx in remaining_indices:
                prob_score = probabilities[idx]
                
                # ì´ë¯¸ ì„ íƒëœ ë²ˆí˜¸ì™€ì˜ ë‹¤ì–‘ì„± ì ìˆ˜
                diversity_score = self._calculate_diversity_score(
                    idx + 1, selected
                )
                
                combined_score = (
                    prob_score * (1 - diversity_factor) + 
                    diversity_score * diversity_factor
                )
                scores.append(combined_score)
                
            # ìµœê³  ì ìˆ˜ ë²ˆí˜¸ ì„ íƒ
            best_remaining = remaining_indices[np.argmax(scores)]
            selected.append(best_remaining + 1)
            remaining_indices.remove(best_remaining)
            
        return selected[:6]
        
    def _calculate_diversity_score(self, candidate: int, selected: List[int]) -> float:
        """ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°"""
        if not selected:
            return 1.0
            
        diversity_score = 1.0
        
        for num in selected:
            # ìˆ«ì ê°„ê²© ë‹¤ì–‘ì„±
            distance = abs(candidate - num)
            if distance < 5:  # ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ì ìˆ˜ ê°ì†Œ
                diversity_score *= 0.7
            elif distance > 30:  # ë„ˆë¬´ ë©€ì–´ë„ ì•½ê°„ ê°ì†Œ
                diversity_score *= 0.9
                
        # í™€ì§ ê· í˜•
        selected_odd_count = sum(1 for n in selected if n % 2 == 1)
        if candidate % 2 == 1:  # í™€ìˆ˜
            if selected_odd_count >= 4:  # ì´ë¯¸ í™€ìˆ˜ê°€ ë§ìœ¼ë©´
                diversity_score *= 0.8
        else:  # ì§ìˆ˜
            if len(selected) - selected_odd_count >= 3:  # ì´ë¯¸ ì§ìˆ˜ê°€ ë§ìœ¼ë©´
                diversity_score *= 0.8
                
        return diversity_score
        
    def _analyze_pattern_features(self, 
                                pattern_features: np.ndarray,
                                machine_type: str) -> Dict[str, Any]:
        """íŒ¨í„´ íŠ¹ì„± ë¶„ì„"""
        
        # íŒ¨í„´ íŠ¹ì„± ë²¡í„°ë¥¼ í•´ì„ ê°€ëŠ¥í•œ ì •ë³´ë¡œ ë³€í™˜
        feature_analysis = {
            'pattern_strength': float(np.mean(np.abs(pattern_features))),
            'pattern_complexity': float(np.std(pattern_features)),
            'dominant_features': pattern_features.argsort()[-5:].tolist(),
            'pattern_signature': self._generate_pattern_signature(pattern_features)
        }
        
        # í˜¸ê¸°ë³„ íŠ¹ì„± ì ìš©
        machine_specific = self._apply_machine_specific_analysis(
            feature_analysis, machine_type
        )
        feature_analysis.update(machine_specific)
        
        return feature_analysis
        
    def _generate_pattern_signature(self, features: np.ndarray) -> str:
        """íŒ¨í„´ ì‹œê·¸ë‹ˆì²˜ ìƒì„±"""
        # íŠ¹ì„± ë²¡í„°ë¥¼ í•´ì‹œë¡œ ë³€í™˜
        feature_hash = hash(tuple(features.round(3)))
        return f"pattern_{abs(feature_hash) % 10000:04d}"
        
    def _apply_machine_specific_analysis(self, 
                                       base_analysis: Dict[str, Any],
                                       machine_type: str) -> Dict[str, Any]:
        """í˜¸ê¸°ë³„ íŠ¹í™” ë¶„ì„"""
        
        machine_modifiers = {
            '1í˜¸ê¸°': {
                'conservative_factor': 0.8,  # ë³´ìˆ˜ì  ì„±í–¥
                'high_frequency_bias': 0.3,  # ê³ ë¹ˆë„ ë²ˆí˜¸ ì„ í˜¸
                'explanation': 'ì‹ ì¤‘í•œ ì „ëµê°€ íŒ¨í„´ ê°ì§€'
            },
            '2í˜¸ê¸°': {
                'balance_factor': 1.0,       # ê· í˜• ì¤‘ì‹œ
                'harmony_preference': 0.4,   # ì¡°í™”ë¡œìš´ ì¡°í•©
                'explanation': 'ì™„ë²½í•œ ì¡°í™” íŒ¨í„´ ì¶”êµ¬'
            },
            '3í˜¸ê¸°': {
                'creativity_factor': 1.2,    # ì°½ì˜ì  ì„±í–¥
                'diversity_bonus': 0.5,      # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
                'explanation': 'ì°½ì¡°ì  í˜ì‹  íŒ¨í„´ íƒì§€'
            }
        }
        
        modifier = machine_modifiers.get(machine_type, machine_modifiers['2í˜¸ê¸°'])
        
        return {
            'machine_specific_analysis': modifier,
            'pattern_adaptation': f"íŒ¨í„´ì´ {machine_type} íŠ¹ì„±ì— ë§ê²Œ ì¡°ì •ë¨"
        }
        
    def _calculate_pattern_confidence(self,
                                    selected_numbers: List[int],
                                    probabilities: np.ndarray,
                                    pattern_analysis: Dict[str, Any]) -> float:
        """íŒ¨í„´ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        # ì„ íƒëœ ë²ˆí˜¸ë“¤ì˜ í‰ê·  í™•ë¥ 
        avg_prob = np.mean([probabilities[num-1] for num in selected_numbers])
        
        # íŒ¨í„´ ê°•ë„
        pattern_strength = pattern_analysis['pattern_strength']
        
        # ë‹¤ì–‘ì„± ì ìˆ˜
        diversity_score = self._calculate_diversity_score_for_set(selected_numbers)
        
        # ì¢…í•© ì‹ ë¢°ë„
        confidence = (
            avg_prob * 0.5 +
            pattern_strength * 0.3 +
            diversity_score * 0.2
        )
        
        return min(max(confidence, 0.0), 1.0)
        
    def _calculate_diversity_score_for_set(self, numbers: List[int]) -> float:
        """ë²ˆí˜¸ ì„¸íŠ¸ì˜ ë‹¤ì–‘ì„± ì ìˆ˜"""
        if len(numbers) != 6:
            return 0.0
            
        # í™€ì§ ê· í˜•
        odd_count = sum(1 for n in numbers if n % 2 == 1)
        odd_balance = 1.0 - abs(odd_count - 3) / 3.0
        
        # ê³ ì € ê· í˜• (1-22 ì €ìˆ˜, 23-45 ê³ ìˆ˜)
        high_count = sum(1 for n in numbers if n >= 23)
        high_balance = 1.0 - abs(high_count - 3) / 3.0
        
        # ë²ˆí˜¸ ë¶„ì‚°
        spread = np.std(numbers) / 15.0  # ì •ê·œí™”
        
        return (odd_balance + high_balance + spread) / 3.0
        
    async def _analyze_base_patterns(self):
        """ê¸°ë³¸ íŒ¨í„´ ë¶„ì„"""
        self.logger.info("ğŸ” Analyzing base patterns...")
        
        try:
            # ê° í˜¸ê¸°ë³„ ê¸°ë³¸ íŒ¨í„´ ë¶„ì„
            for machine_type in ['1í˜¸ê¸°', '2í˜¸ê¸°', '3í˜¸ê¸°']:
                pattern_key = f"base_patterns_{machine_type}"
                
                # ìºì‹œ í™•ì¸
                cached = await self.memory_manager.get_pattern_cache(
                    pattern_key, machine_type
                )
                
                if not cached:
                    # ìƒˆë¡œ ë¶„ì„
                    historical_data = await self._load_historical_data(machine_type, 100)
                    if historical_data:
                        patterns = self._extract_base_patterns(historical_data)
                        
                        # ìºì‹œì— ì €ì¥
                        await self.memory_manager.store_pattern_cache(
                            pattern_key, machine_type, patterns, 0.8, 48  # 48ì‹œê°„
                        )
                        
                        self.discovered_patterns[machine_type] = patterns
                        
        except Exception as e:
            self.logger.error(f"âŒ Base pattern analysis failed: {e}")
            
    def _extract_base_patterns(self, historical_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = {
            'number_frequency': {},
            'pair_frequency': {},
            'sequence_patterns': [],
            'statistical_patterns': {}
        }
        
        # ë²ˆí˜¸ ë¹ˆë„ ë¶„ì„
        all_numbers = []
        for record in historical_data:
            all_numbers.extend(record['1ë“±_ë‹¹ì²¨ë²ˆí˜¸'])
            
        for num in range(1, 46):
            patterns['number_frequency'][num] = all_numbers.count(num)
            
        # í˜ì–´ ë¹ˆë„ ë¶„ì„
        for record in historical_data:
            numbers = record['1ë“±_ë‹¹ì²¨ë²ˆí˜¸']
            for i in range(len(numbers)):
                for j in range(i+1, len(numbers)):
                    pair = tuple(sorted([numbers[i], numbers[j]]))
                    if pair not in patterns['pair_frequency']:
                        patterns['pair_frequency'][pair] = 0
                    patterns['pair_frequency'][pair] += 1
                    
        # í†µê³„ì  íŒ¨í„´
        patterns['statistical_patterns'] = {
            'avg_odd_count': np.mean([
                sum(1 for n in r['1ë“±_ë‹¹ì²¨ë²ˆí˜¸'] if n % 2 == 1) 
                for r in historical_data
            ]),
            'avg_high_count': np.mean([
                sum(1 for n in r['1ë“±_ë‹¹ì²¨ë²ˆí˜¸'] if n >= 23)
                for r in historical_data
            ]),
            'avg_total_sum': np.mean([r['ì´í•©'] for r in historical_data]),
            'avg_ac_value': np.mean([r['ACê°’'] for r in historical_data])
        }
        
        return patterns
        
    async def _load_pretrained_weights(self):
        """ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        try:
            # ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ
            weights_path = f"models/pattern_analyzer_{self.agent_id}.pth"
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íŒŒì¼ ì¡´ì¬ í™•ì¸ í›„ ë¡œë“œ
            self.logger.info("ğŸ”„ No pretrained weights found, using random initialization")
        except Exception as e:
            self.logger.warning(f"âš ï¸  Could not load pretrained weights: {e}")
            
    async def _generate_metadata(self, 
                               machine_type: str,
                               predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        return {
            'model_type': 'PyTorch Transformer',
            'sequence_length': self.sequence_length,
            'prediction_method': 'neural_pattern_recognition',
            'machine_specialization': machine_type,
            'pattern_signatures': [p['pattern_analysis']['pattern_signature'] for p in predictions],
            'avg_pattern_strength': np.mean([
                p['pattern_analysis']['pattern_strength'] for p in predictions
            ]),
            'model_device': str(self.device),
            'discovered_patterns_count': len(self.discovered_patterns.get(machine_type, {}))
        }
    
    def _get_historical_numbers(self, machine_type: str) -> List[List[int]]:
        """ë°ì´í„° ë¡œë”ì—ì„œ ê³¼ê±° ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(self, 'data_loader') and self.data_loader:
                if machine_type == "ì „ì²´":
                    draws = self.data_loader.get_all_draws()
                else:
                    draws = self.data_loader.get_draws_by_machine(machine_type)
                
                # ë²ˆí˜¸ë§Œ ì¶”ì¶œ
                historical_numbers = []
                for draw in draws[-50:]:  # ìµœê·¼ 50íšŒ
                    historical_numbers.append(draw.numbers)
                    
                return historical_numbers
            else:
                return []
        except Exception as e:
            self.logger.error(f"âŒ Failed to get historical data: {e}")
            return []
    
    def _analyze_pattern_for_numbers(self, numbers: List[int], machine_type: str) -> Dict[str, Any]:
        """íŠ¹ì • ë²ˆí˜¸ ì¡°í•©ì˜ íŒ¨í„´ ë¶„ì„"""
        try:
            odd_count = sum(1 for n in numbers if n % 2 == 1)
            high_count = sum(1 for n in numbers if n >= 23)
            digit_sum = sum(numbers)
            
            return {
                'pattern_strength': 0.7,  # ê¸°ë³¸ê°’
                'pattern_complexity': 0.5,
                'odd_count': odd_count,
                'high_count': high_count,
                'digit_sum': digit_sum,
                'machine_specific_analysis': {
                    'explanation': f'íŒ¨í„´ì´ {machine_type} íŠ¹ì„±ì— ë§ê²Œ ë¶„ì„ë¨'
                },
                'pattern_signature': f"pattern_{hash(tuple(numbers)) % 10000:04d}"
            }
        except:
            return {'pattern_strength': 0.5, 'pattern_complexity': 0.5}
    
    async def _fallback_predictions(self, historical_data: List[List[int]], sets_count: int, machine_type: str) -> List[Dict[str, Any]]:
        """ëª¨ë¸ ì—†ì„ ë•Œ ì‚¬ìš©í•  fallback ì˜ˆì¸¡"""
        predictions = []
        
        try:
            # ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì˜ˆì¸¡
            all_numbers = []
            for draw in historical_data[-20:]:  # ìµœê·¼ 20íšŒ
                all_numbers.extend(draw)
                
            # ë¹ˆë„ ê³„ì‚°
            frequency = {}
            for num in range(1, 46):
                frequency[num] = all_numbers.count(num)
                
            for i in range(sets_count):
                # ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ ì„ íƒ
                if frequency:
                    weights = [frequency.get(num, 0) + 1 for num in range(1, 46)]
                    total_weight = sum(weights)
                    probabilities = [w / total_weight for w in weights]
                    
                    selected = np.random.choice(
                        range(1, 46), size=6, replace=False, p=probabilities
                    )
                    numbers = sorted(selected.tolist())
                else:
                    numbers = sorted(np.random.choice(range(1, 46), 6, replace=False))
                
                predictions.append({
                    'set_number': i + 1,
                    'numbers': numbers,
                    'confidence_score': 0.4,
                    'pattern_analysis': self._analyze_pattern_for_numbers(numbers, machine_type),
                    'selection_method': 'statistical_fallback',
                    'diversity_factor': self.prediction_diversity
                })
                
        except Exception as e:
            self.logger.error(f"âŒ Fallback prediction failed: {e}")
            # ì™„ì „ ëœë¤ fallback
            for i in range(sets_count):
                numbers = sorted(np.random.choice(range(1, 46), 6, replace=False))
                predictions.append({
                    'set_number': i + 1,
                    'numbers': numbers.tolist(),
                    'confidence_score': 0.2,
                    'pattern_analysis': self._analyze_pattern_for_numbers(numbers, machine_type),
                    'selection_method': 'random_fallback',
                    'diversity_factor': self.prediction_diversity
                })
                
        return predictions