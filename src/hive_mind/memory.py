# ğŸ§  Hive Memory Manager - Advanced Memory Management System
# SQLite + Redis hybrid memory architecture with intelligent caching

import asyncio
import sqlite3
import json
import logging
import aiosqlite
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
import hashlib
import pickle
import redis.asyncio as redis

@dataclass
class MemoryConfig:
    """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„¤ì •"""
    db_path: str = "data/hive_memory.db"
    cache_enabled: bool = True
    redis_url: str = "redis://localhost:6379"
    cache_ttl: int = 3600  # 1ì‹œê°„
    max_cache_size_mb: int = 256
    compression_enabled: bool = True

class HiveMemoryManager:
    """
    ì°¨ì„¸ëŒ€ í•˜ì´ë¸Œ-ë§ˆì¸ë“œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
    
    Claude-Flowì˜ memory.dbë¥¼ í™•ì¥í•œ ê³ ë„í™”ëœ ì‹œìŠ¤í…œ:
    - ê³„ì¸µí™”ëœ ë©”ëª¨ë¦¬ (ë‹¨ê¸°/ì¥ê¸°)
    - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ ë°ì´í„° ë¶„ë¥˜
    - ì§€ëŠ¥í˜• ìºì‹± ë° ì••ì¶•
    - ì‹¤ì‹œê°„ ë¶„ì„ ë° ë°±ì—…
    """
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.db_path = Path(config.db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.db_connection: Optional[aiosqlite.Connection] = None
        self.redis_client: Optional[redis.Redis] = None
        
        self.logger = logging.getLogger(__name__)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self._memory_usage = {
            'cache_size_mb': 0,
            'db_size_mb': 0,
            'total_records': 0
        }
        
    async def initialize(self):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.logger.info("ğŸ§  Initializing Hive Memory Manager...")
        
        # SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db_connection = await aiosqlite.connect(str(self.db_path))
        
        # í…Œì´ë¸” ìƒì„±
        await self._create_tables()
        
        # Redis ìºì‹œ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
        if self.config.cache_enabled:
            try:
                self.redis_client = redis.from_url(
                    self.config.redis_url,
                    decode_responses=False  # ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì§€ì›
                )
                await self.redis_client.ping()
                self.logger.info("âœ… Redis cache connected")
            except Exception as e:
                self.logger.warning(f"âš ï¸  Redis unavailable, using in-memory cache: {e}")
                self.redis_client = None
                
        await self._update_memory_usage()
        self.logger.info("âœ… Hive Memory Manager initialized")
        
    async def shutdown(self):
        """ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        self.logger.info("ğŸ”„ Shutting down Hive Memory Manager...")
        
        if self.redis_client:
            await self.redis_client.close()
            
        if self.db_connection:
            await self.db_connection.close()
            
        self.logger.info("âœ… Memory system shutdown complete")
        
    async def _create_tables(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
        
        # 1. ê¸°ë³¸ ë¡œë˜ ì¶”ì²¨ ë°ì´í„°
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS lottery_draws (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                draw_round INTEGER NOT NULL,
                machine_type VARCHAR(10) NOT NULL,
                winning_numbers TEXT NOT NULL,
                draw_date DATE,
                odd_even_ratio VARCHAR(10),
                high_low_ratio VARCHAR(10),
                ac_value INTEGER,
                last_digit_sum INTEGER,
                total_sum INTEGER,
                special_memo TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(draw_round, machine_type)
            )
        """)
        
        # 2. ì˜ˆì¸¡ ê²°ê³¼ ë¡œê·¸
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS prediction_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                request_id VARCHAR(50) NOT NULL,
                machine_type VARCHAR(10) NOT NULL,
                algorithm VARCHAR(50),
                predictions TEXT NOT NULL,
                confidence_score REAL,
                agent_results TEXT,
                explanation TEXT,
                performance_metrics TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # 3. ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë°ì´í„°
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS agent_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                agent_id VARCHAR(50) NOT NULL,
                agent_type VARCHAR(50),
                tasks_processed INTEGER DEFAULT 0,
                success_count INTEGER DEFAULT 0,
                error_count INTEGER DEFAULT 0,
                avg_response_time REAL DEFAULT 0.0,
                success_rate REAL DEFAULT 0.0,
                capabilities TEXT,
                last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(agent_id)
            )
        """)
        
        # 4. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS system_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_name VARCHAR(100) NOT NULL,
                metric_value REAL,
                metadata TEXT,
                recorded_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # 5. ì‘ì—… ì‹¤í–‰ ê¸°ë¡
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS task_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id VARCHAR(50) NOT NULL,
                task_type VARCHAR(50),
                status VARCHAR(20),
                assigned_agents TEXT,
                result TEXT,
                error_message TEXT,
                processing_time_ms REAL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                completed_at DATETIME
            )
        """)
        
        # 6. íŒ¨í„´ ë¶„ì„ ìºì‹œ
        await self.db_connection.execute("""
            CREATE TABLE IF NOT EXISTS pattern_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_key VARCHAR(100) NOT NULL,
                machine_type VARCHAR(10),
                pattern_data TEXT NOT NULL,
                confidence_score REAL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                expires_at DATETIME,
                UNIQUE(pattern_key, machine_type)
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_lottery_draws_round_machine ON lottery_draws(draw_round, machine_type)",
            "CREATE INDEX IF NOT EXISTS idx_prediction_logs_request ON prediction_logs(request_id)",
            "CREATE INDEX IF NOT EXISTS idx_agent_performance_id ON agent_performance(agent_id)",
            "CREATE INDEX IF NOT EXISTS idx_task_history_id ON task_history(task_id)",
            "CREATE INDEX IF NOT EXISTS idx_pattern_cache_key ON pattern_cache(pattern_key, machine_type)"
        ]
        
        for index in indexes:
            await self.db_connection.execute(index)
            
        await self.db_connection.commit()
        
    # === ë¡œë˜ ë°ì´í„° ê´€ë¦¬ ===
    
    async def store_lottery_data(self, lottery_data: List[Dict[str, Any]]):
        """ë¡œë˜ ì¶”ì²¨ ë°ì´í„° ì €ì¥"""
        for record in lottery_data:
            await self.db_connection.execute("""
                INSERT OR REPLACE INTO lottery_draws 
                (draw_round, machine_type, winning_numbers, draw_date,
                 odd_even_ratio, high_low_ratio, ac_value, 
                 last_digit_sum, total_sum, special_memo, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (
                record['íšŒì°¨'],
                record['í˜¸ê¸°'],
                json.dumps(record['1ë“±_ë‹¹ì²¨ë²ˆí˜¸']),
                record.get('ì¶”ì²¨ì¼'),
                record['í™€ì§_ë¹„ìœ¨'],
                record['ê³ ì €_ë¹„ìœ¨'],
                record['ACê°’'],
                record['ëìˆ˜í•©'],
                record['ì´í•©'],
                record.get('íŠ¹ë³„_ë©”ëª¨')
            ))
            
        await self.db_connection.commit()
        self.logger.info(f"ğŸ’¾ Stored {len(lottery_data)} lottery records")
        
    async def get_lottery_data(self, 
                              machine_type: Optional[str] = None,
                              limit: int = 50) -> List[Dict[str, Any]]:
        """ë¡œë˜ ì¶”ì²¨ ë°ì´í„° ì¡°íšŒ"""
        query = """
            SELECT draw_round, machine_type, winning_numbers, draw_date,
                   odd_even_ratio, high_low_ratio, ac_value,
                   last_digit_sum, total_sum, special_memo
            FROM lottery_draws
        """
        params = []
        
        if machine_type:
            query += " WHERE machine_type = ?"
            params.append(machine_type)
            
        query += " ORDER BY draw_round DESC LIMIT ?"
        params.append(limit)
        
        cursor = await self.db_connection.execute(query, params)
        rows = await cursor.fetchall()
        
        results = []
        for row in rows:
            results.append({
                'íšŒì°¨': row[0],
                'í˜¸ê¸°': row[1],
                '1ë“±_ë‹¹ì²¨ë²ˆí˜¸': json.loads(row[2]),
                'ì¶”ì²¨ì¼': row[3],
                'í™€ì§_ë¹„ìœ¨': row[4],
                'ê³ ì €_ë¹„ìœ¨': row[5],
                'ACê°’': row[6],
                'ëìˆ˜í•©': row[7],
                'ì´í•©': row[8],
                'íŠ¹ë³„_ë©”ëª¨': row[9]
            })
            
        return results
        
    # === ì˜ˆì¸¡ ê²°ê³¼ ê´€ë¦¬ ===
    
    async def store_prediction_result(self, result: Dict[str, Any]):
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        await self.db_connection.execute("""
            INSERT INTO prediction_logs 
            (request_id, machine_type, algorithm, predictions, 
             confidence_score, agent_results, explanation, performance_metrics)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            result['request_id'],
            result['machine_type'],
            result.get('algorithm'),
            json.dumps(result['predictions']),
            result.get('confidence_score'),
            json.dumps(result.get('agent_results', {})),
            json.dumps(result.get('explanation', [])),
            json.dumps(result.get('performance_metrics', {}))
        ))
        
        await self.db_connection.commit()
        
    async def get_prediction_history(self, 
                                   machine_type: Optional[str] = None,
                                   limit: int = 20) -> List[Dict[str, Any]]:
        """ì˜ˆì¸¡ ê¸°ë¡ ì¡°íšŒ"""
        query = """
            SELECT request_id, machine_type, algorithm, predictions,
                   confidence_score, created_at
            FROM prediction_logs
        """
        params = []
        
        if machine_type:
            query += " WHERE machine_type = ?"
            params.append(machine_type)
            
        query += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)
        
        cursor = await self.db_connection.execute(query, params)
        rows = await cursor.fetchall()
        
        results = []
        for row in rows:
            results.append({
                'request_id': row[0],
                'machine_type': row[1],
                'algorithm': row[2],
                'predictions': json.loads(row[3]) if row[3] else [],
                'confidence_score': row[4],
                'created_at': row[5]
            })
            
        return results
        
    # === ì—ì´ì „íŠ¸ ì„±ëŠ¥ ê´€ë¦¬ ===
    
    async def store_agent_performance(self, agent_id: str, data: Dict[str, Any]):
        """ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë°ì´í„° ì €ì¥"""
        metrics = data['metrics']
        
        await self.db_connection.execute("""
            INSERT OR REPLACE INTO agent_performance 
            (agent_id, agent_type, tasks_processed, success_count, error_count,
             avg_response_time, success_rate, capabilities, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """, (
            agent_id,
            data['agent_type'],
            metrics['tasks_processed'],
            metrics['success_count'],
            metrics['error_count'],
            metrics['avg_response_time'],
            metrics['success_rate'],
            json.dumps(data['capabilities'])
        ))
        
        await self.db_connection.commit()
        
    async def get_agent_performance(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ"""
        
        # ìºì‹œ í™•ì¸
        if self.redis_client:
            cache_key = f"agent_perf:{agent_id}"
            cached = await self.redis_client.get(cache_key)
            if cached:
                return json.loads(cached.decode('utf-8'))
        
        cursor = await self.db_connection.execute("""
            SELECT tasks_processed, success_count, error_count,
                   avg_response_time, success_rate, capabilities, last_updated
            FROM agent_performance 
            WHERE agent_id = ?
        """, (agent_id,))
        
        row = await cursor.fetchone()
        if not row:
            return None
            
        result = {
            'tasks_processed': row[0],
            'success_count': row[1], 
            'error_count': row[2],
            'avg_response_time_ms': row[3],
            'success_rate': row[4],
            'avg_confidence': row[4],  # ì„ì‹œë¡œ success_rate ì‚¬ìš©
            'capabilities': json.loads(row[5]) if row[5] else [],
            'last_updated': row[6]
        }
        
        # ìºì‹œì— ì €ì¥
        if self.redis_client:
            await self.redis_client.setex(
                cache_key, 
                self.config.cache_ttl,
                json.dumps(result)
            )
            
        return result
        
    # === ì‘ì—… ê¸°ë¡ ê´€ë¦¬ ===
    
    async def store_task_result(self, task_data: Dict[str, Any]):
        """ì‘ì—… ê²°ê³¼ ì €ì¥"""
        await self.db_connection.execute("""
            INSERT INTO task_history 
            (task_id, task_type, status, assigned_agents, result, 
             error_message, processing_time_ms, completed_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            task_data['id'],
            task_data.get('type'),
            task_data['status'],
            json.dumps(task_data.get('assigned_agents', [])),
            json.dumps(task_data.get('result')),
            task_data.get('error'),
            self._calculate_processing_time(task_data),
            task_data.get('completed_at')
        ))
        
        await self.db_connection.commit()
        
    def _calculate_processing_time(self, task_data: Dict[str, Any]) -> Optional[float]:
        """ì‘ì—… ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°"""
        if task_data.get('started_at') and task_data.get('completed_at'):
            try:
                start = datetime.fromisoformat(task_data['started_at'])
                end = datetime.fromisoformat(task_data['completed_at'])
                return (end - start).total_seconds() * 1000
            except:
                pass
        return None
        
    # === íŒ¨í„´ ìºì‹œ ê´€ë¦¬ ===
    
    async def store_pattern_cache(self,
                                 pattern_key: str,
                                 machine_type: str,
                                 pattern_data: Dict[str, Any],
                                 confidence_score: float,
                                 ttl_hours: int = 24):
        """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ìºì‹œ"""
        expires_at = datetime.now() + timedelta(hours=ttl_hours)
        
        await self.db_connection.execute("""
            INSERT OR REPLACE INTO pattern_cache 
            (pattern_key, machine_type, pattern_data, confidence_score, expires_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            pattern_key,
            machine_type,
            json.dumps(pattern_data),
            confidence_score,
            expires_at.isoformat()
        ))
        
        await self.db_connection.commit()
        
    async def get_pattern_cache(self,
                               pattern_key: str,
                               machine_type: str) -> Optional[Dict[str, Any]]:
        """íŒ¨í„´ ìºì‹œ ì¡°íšŒ"""
        cursor = await self.db_connection.execute("""
            SELECT pattern_data, confidence_score, created_at
            FROM pattern_cache 
            WHERE pattern_key = ? AND machine_type = ? 
            AND expires_at > CURRENT_TIMESTAMP
        """, (pattern_key, machine_type))
        
        row = await cursor.fetchone()
        if not row:
            return None
            
        return {
            'data': json.loads(row[0]),
            'confidence_score': row[1],
            'created_at': row[2]
        }
        
    # === ì‹œìŠ¤í…œ í†µê³„ ë° ëª¨ë‹ˆí„°ë§ ===
    
    async def get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜ (0.0-1.0)"""
        await self._update_memory_usage()
        
        # ê°„ë‹¨í•œ ì‚¬ìš©ë¥  ê³„ì‚° (ì´ 256MB ê¸°ì¤€)
        total_mb = self._memory_usage['cache_size_mb'] + self._memory_usage['db_size_mb']
        max_mb = self.config.max_cache_size_mb + 512  # DB ìµœëŒ€ 512MB
        
        return min(total_mb / max_mb, 1.0)
        
    async def _update_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        try:
            # DB íŒŒì¼ í¬ê¸°
            if self.db_path.exists():
                self._memory_usage['db_size_mb'] = self.db_path.stat().st_size / (1024 * 1024)
                
            # ì „ì²´ ë ˆì½”ë“œ ìˆ˜
            cursor = await self.db_connection.execute(
                "SELECT COUNT(*) FROM sqlite_master WHERE type='table'"
            )
            self._memory_usage['total_records'] = (await cursor.fetchone())[0]
            
            # Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì¶”ì •)
            if self.redis_client:
                try:
                    info = await self.redis_client.info('memory')
                    self._memory_usage['cache_size_mb'] = info.get('used_memory', 0) / (1024 * 1024)
                except:
                    pass
                    
        except Exception as e:
            self.logger.error(f"âŒ Failed to update memory usage: {e}")
            
    async def get_system_statistics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        await self._update_memory_usage()
        
        # ê¸°ë³¸ í†µê³„ ì¿¼ë¦¬ë“¤
        stats = {}
        
        # ë¡œë˜ ë°ì´í„° í†µê³„
        cursor = await self.db_connection.execute(
            "SELECT COUNT(*) FROM lottery_draws"
        )
        stats['total_lottery_draws'] = (await cursor.fetchone())[0]
        
        # ì˜ˆì¸¡ ê¸°ë¡ í†µê³„
        cursor = await self.db_connection.execute(
            "SELECT COUNT(*) FROM prediction_logs"
        )
        stats['total_predictions'] = (await cursor.fetchone())[0]
        
        # ì—ì´ì „íŠ¸ ìˆ˜
        cursor = await self.db_connection.execute(
            "SELECT COUNT(*) FROM agent_performance"
        )
        stats['registered_agents'] = (await cursor.fetchone())[0]
        
        # ìµœê·¼ 24ì‹œê°„ í™œë™
        yesterday = datetime.now() - timedelta(days=1)
        cursor = await self.db_connection.execute(
            "SELECT COUNT(*) FROM prediction_logs WHERE created_at >= ?",
            (yesterday.isoformat(),)
        )
        stats['predictions_24h'] = (await cursor.fetchone())[0]
        
        return {
            'statistics': stats,
            'memory_usage': self._memory_usage,
            'memory_usage_ratio': await self.get_memory_usage()
        }
        
    async def cleanup_old_data(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        cutoff_str = cutoff_date.isoformat()
        
        # ì˜¤ë˜ëœ ì˜ˆì¸¡ ë¡œê·¸ ì‚­ì œ
        await self.db_connection.execute(
            "DELETE FROM prediction_logs WHERE created_at < ?",
            (cutoff_str,)
        )
        
        # ë§Œë£Œëœ íŒ¨í„´ ìºì‹œ ì‚­ì œ
        await self.db_connection.execute(
            "DELETE FROM pattern_cache WHERE expires_at < CURRENT_TIMESTAMP"
        )
        
        # ì˜¤ë˜ëœ ì‘ì—… ê¸°ë¡ ì‚­ì œ
        await self.db_connection.execute(
            "DELETE FROM task_history WHERE created_at < ?",
            (cutoff_str,)
        )
        
        await self.db_connection.commit()
        self.logger.info(f"ğŸ§¹ Cleaned up data older than {days_to_keep} days")
        
    async def backup_database(self, backup_path: str):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        backup_file = Path(backup_path)
        backup_file.parent.mkdir(parents=True, exist_ok=True)
        
        # SQLite ë°±ì—…
        async with aiosqlite.connect(str(backup_file)) as backup_db:
            await self.db_connection.backup(backup_db)
            
        self.logger.info(f"ğŸ’¾ Database backed up to {backup_path}")