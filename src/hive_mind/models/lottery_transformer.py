# ğŸ¤– Lottery Pattern Transformer Model
# Simplified but functional transformer for lottery number pattern recognition

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import List, Tuple, Optional, Dict, Any
import math

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class LotteryTransformer(nn.Module):
    """
    ë¡œë˜ ë²ˆí˜¸ íŒ¨í„´ ì¸ì‹ìš© íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
    
    ê°„ì†Œí™”ëœ êµ¬ì¡°ë¡œ ì‹¤ì œ ë™ì‘ ê°€ëŠ¥í•œ ëª¨ë¸
    """
    
    def __init__(self, 
                 vocab_size: int = 45,  # 1-45 ë²ˆí˜¸
                 d_model: int = 64,
                 nhead: int = 8,
                 num_layers: int = 4,
                 dim_feedforward: int = 256,
                 dropout: float = 0.1,
                 max_seq_len: int = 20):
        
        super().__init__()
        
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # ì„ë² ë”© ë ˆì´ì–´ (ë²ˆí˜¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜)
        self.number_embedding = nn.Embedding(vocab_size + 1, d_model)  # +1 for padding
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # ì¶œë ¥ í—¤ë“œ (45ê°œ ë²ˆí˜¸ í™•ë¥ )
        self.output_head = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, vocab_size)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()
        
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.xavier_uniform_(module.weight)
                
    def forward(self, x, mask=None):
        """
        ìˆœì „íŒŒ
        x: (batch_size, seq_len) - ê° ì‹œí€€ìŠ¤ëŠ” 6ê°œ ë²ˆí˜¸ì˜ sequence
        """
        # ì„ë² ë”©
        x = self.number_embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”©
        x = self.transformer(x, src_key_padding_mask=mask)
        
        # í‰ê·  í’€ë§ (ì‹œí€€ìŠ¤ ì°¨ì›ì—ì„œ)
        if mask is not None:
            mask_expanded = mask.unsqueeze(-1).expand_as(x)
            x = x.masked_fill(mask_expanded, 0)
            lengths = (~mask).sum(dim=1, keepdim=True).float()
            x = x.sum(dim=1) / lengths
        else:
            x = x.mean(dim=1)
            
        # ì¶œë ¥ í™•ë¥ 
        logits = self.output_head(x)
        return logits
        
    def predict_numbers(self, 
                       history: List[List[int]], 
                       num_sets: int = 1,
                       diversity_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """
        ë²ˆí˜¸ ì˜ˆì¸¡
        
        Args:
            history: ê³¼ê±° ì¶”ì²¨ ë²ˆí˜¸ë“¤ [[1,2,3,4,5,6], [7,8,9,10,11,12], ...]
            num_sets: ì˜ˆì¸¡í•  ì„¸íŠ¸ ìˆ˜
            diversity_threshold: ë‹¤ì–‘ì„± ì„ê³„ê°’
        """
        self.eval()
        
        with torch.no_grad():
            # ì…ë ¥ ì¤€ë¹„
            if len(history) == 0:
                # ê³¼ê±° ë°ì´í„° ì—†ìœ¼ë©´ ëœë¤
                return self._generate_random_predictions(num_sets)
                
            # ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨)
            recent_history = history[-10:] if len(history) > 10 else history
            
            # í…ì„œ ë³€í™˜
            input_seqs = []
            for draw in recent_history:
                # ê° ì¶”ì²¨ì„ 6ê°œ ë²ˆí˜¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
                input_seqs.extend([num for num in draw])
                
            # íŒ¨ë”© ë° ë°°ì¹˜ ì²˜ë¦¬
            max_len = min(len(input_seqs), 60)  # ìµœëŒ€ 10íšŒ * 6ë²ˆí˜¸
            if len(input_seqs) < max_len:
                input_seqs.extend([0] * (max_len - len(input_seqs)))  # íŒ¨ë”©
            else:
                input_seqs = input_seqs[-max_len:]  # ìµœê·¼ ë°ì´í„°ë§Œ
                
            x = torch.tensor([input_seqs], dtype=torch.long)
            
            # ì˜ˆì¸¡
            logits = self.forward(x)
            probabilities = F.softmax(logits, dim=-1).squeeze(0)  # (45,)
            
            # ë‹¤ì–‘í•œ ì˜ˆì¸¡ ìƒì„±
            predictions = []
            used_combinations = set()
            
            for _ in range(num_sets):
                # Top-k ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
                top_k = min(15, len(probabilities))  # ìƒìœ„ 15ê°œì—ì„œ ì„ íƒ
                top_probs, top_indices = torch.topk(probabilities, top_k)
                
                # ì˜¨ë„ ê¸°ë°˜ ìƒ˜í”Œë§
                temperature = 1.0 + (_ * 0.3)  # ì ì§„ì ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¦ê°€
                scaled_probs = F.softmax(top_probs / temperature, dim=-1)
                
                selected_numbers = []
                available_indices = list(range(top_k))
                
                # 6ê°œ ë²ˆí˜¸ ì„ íƒ
                for _ in range(6):
                    if not available_indices:
                        break
                        
                    # í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§
                    weights = [scaled_probs[i].item() for i in available_indices]
                    total_weight = sum(weights)
                    if total_weight == 0:
                        choice_idx = np.random.choice(available_indices)
                    else:
                        choice_idx = np.random.choice(
                            available_indices, 
                            p=[w/total_weight for w in weights]
                        )
                    
                    selected_idx = available_indices[choice_idx]
                    number = top_indices[selected_idx].item() + 1  # 1-based
                    selected_numbers.append(number)
                    
                    # ì„ íƒëœ ë²ˆí˜¸ëŠ” ì œê±°
                    available_indices.remove(selected_idx)
                
                # 6ê°œê°€ ì•ˆë˜ë©´ ëœë¤ìœ¼ë¡œ ì±„ìš°ê¸°
                while len(selected_numbers) < 6:
                    remaining = [i for i in range(1, 46) if i not in selected_numbers]
                    if remaining:
                        selected_numbers.append(np.random.choice(remaining))
                    else:
                        break
                
                selected_numbers = sorted(selected_numbers[:6])
                combo_key = tuple(selected_numbers)
                
                # ì¤‘ë³µ ì²´í¬ ë° ì¶”ê°€
                if combo_key not in used_combinations and len(selected_numbers) == 6:
                    used_combinations.add(combo_key)
                    
                    # ì‹ ë¢°ë„ ê³„ì‚°
                    confidence = self._calculate_confidence(
                        selected_numbers, probabilities, history
                    )
                    
                    predictions.append({
                        'numbers': selected_numbers,
                        'confidence': confidence,
                        'method': 'transformer_pattern',
                        'metadata': {
                            'temperature': temperature,
                            'top_probs': top_probs[:6].tolist()
                        }
                    })
                    
            # ì˜ˆì¸¡ì´ ë¶€ì¡±í•˜ë©´ ëœë¤ìœ¼ë¡œ ì±„ìš°ê¸°
            while len(predictions) < num_sets:
                random_pred = self._generate_random_predictions(1)[0]
                if tuple(random_pred['numbers']) not in used_combinations:
                    predictions.append(random_pred)
                    used_combinations.add(tuple(random_pred['numbers']))
                    
            return predictions[:num_sets]
            
    def _calculate_confidence(self, 
                            numbers: List[int], 
                            probabilities: torch.Tensor, 
                            history: List[List[int]]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ëª¨ë¸ í™•ë¥  ê¸°ë°˜ ì‹ ë¢°ë„
            model_confidence = sum(probabilities[num-1].item() for num in numbers) / 6
            
            # ê³¼ê±° íŒ¨í„´ ì¼ì¹˜ë„
            pattern_score = 0.0
            if history:
                recent_numbers = set()
                for draw in history[-5:]:  # ìµœê·¼ 5íšŒ
                    recent_numbers.update(draw)
                    
                overlap = len(set(numbers) & recent_numbers)
                pattern_score = min(overlap / 6, 0.5)  # ìµœëŒ€ 0.5
                
            # í†µê³„ì  ê· í˜•
            odd_count = sum(1 for n in numbers if n % 2 == 1)
            balance_score = 1.0 - abs(odd_count - 3) / 3  # í™€ì§ ê· í˜•
            
            # ì¢…í•© ì‹ ë¢°ë„
            confidence = (model_confidence * 0.5 + 
                         pattern_score * 0.2 + 
                         balance_score * 0.3)
            
            return max(0.1, min(0.9, confidence))
            
        except:
            return 0.5
            
    def _generate_random_predictions(self, num_sets: int) -> List[Dict[str, Any]]:
        """ëœë¤ ì˜ˆì¸¡ ìƒì„± (fallback)"""
        predictions = []
        used_combinations = set()
        
        for _ in range(num_sets):
            while True:
                numbers = sorted(np.random.choice(range(1, 46), 6, replace=False))
                combo_key = tuple(numbers)
                
                if combo_key not in used_combinations:
                    used_combinations.add(combo_key)
                    predictions.append({
                        'numbers': numbers.tolist(),
                        'confidence': 0.3,
                        'method': 'random_fallback',
                        'metadata': {}
                    })
                    break
                    
        return predictions

class LotteryPatternPredictor:
    """
    íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤
    """
    
    def __init__(self, model_config: Dict[str, Any] = None):
        self.config = model_config or {}
        self.model: Optional[LotteryTransformer] = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def initialize(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.model = LotteryTransformer(
                d_model=self.config.get('d_model', 64),
                nhead=self.config.get('nhead', 8),
                num_layers=self.config.get('num_layers', 4),
                dropout=self.config.get('dropout', 0.1)
            )
            
            self.model.to(self.device)
            self.logger.info(f"âœ… Transformer model initialized on {self.device}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Model initialization failed: {e}")
            return False
            
    def predict(self, 
                historical_data: List[List[int]], 
                num_predictions: int = 1,
                **kwargs) -> List[Dict[str, Any]]:
        """ì˜ˆì¸¡ ì‹¤í–‰"""
        if self.model is None:
            self.logger.warning("Model not initialized, using random predictions")
            return self._random_fallback(num_predictions)
            
        try:
            diversity_threshold = kwargs.get('diversity_threshold', 0.7)
            return self.model.predict_numbers(
                historical_data, 
                num_predictions, 
                diversity_threshold
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Prediction failed: {e}")
            return self._random_fallback(num_predictions)
            
    def _random_fallback(self, num_predictions: int) -> List[Dict[str, Any]]:
        """ëœë¤ ì˜ˆì¸¡ (fallback)"""
        predictions = []
        for _ in range(num_predictions):
            numbers = sorted(np.random.choice(range(1, 46), 6, replace=False))
            predictions.append({
                'numbers': numbers.tolist(),
                'confidence': 0.2,
                'method': 'random_fallback',
                'metadata': {}
            })
        return predictions
        
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model:
            torch.save(self.model.state_dict(), path)
            self.logger.info(f"Model saved to {path}")
            
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.model:
            self.model.load_state_dict(torch.load(path, map_location=self.device))
            self.model.eval()
            self.logger.info(f"Model loaded from {path}")
            
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´"""
        if self.model is None:
            return {'status': 'not_initialized'}
            
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'status': 'initialized',
            'device': str(self.device),
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_config': self.config
        }