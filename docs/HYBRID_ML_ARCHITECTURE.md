# HYBRID_ML_ARCHITECTURE.md
# ë³µí•© ML ë¡œë˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "Input Layer"
        A[ì‚¬ìš©ì ìš”ì²­] --> B[FastAPI Router]
        C[res.json<br/>140íšŒì°¨ ë°ì´í„°] --> D[DataLoader]
    end
    
    subgraph "Model Dispatcher Layer"
        B --> E[ëª¨ë¸ ë””ìŠ¤íŒ¨ì²˜]
        D --> E
        E --> F[í˜¸ê¸°ë³„ ë¼ìš°íŒ…]
        F --> G[1í˜¸ê¸° ì „ëµ]
        F --> H[2í˜¸ê¸° ì „ëµ] 
        F --> I[3í˜¸ê¸° ì „ëµ]
    end
    
    subgraph "Parallel ML Inference"
        G --> J[PyTorch<br/>Transformer]
        G --> K[scikit-learn<br/>ì•™ìƒë¸”]
        
        H --> L[PyTorch<br/>Transformer]
        H --> M[scikit-learn<br/>ì•™ìƒë¸”]
        
        I --> N[PyTorch<br/>Transformer]
        I --> O[scikit-learn<br/>ì•™ìƒë¸”]
    end
    
    subgraph "Results Aggregation"
        J --> P[ê²°ê³¼ í†µí•©ê¸°]
        K --> P
        L --> P
        M --> P
        N --> P
        O --> P
        P --> Q[ì•™ìƒë¸” ì „ëµ<br/>ì ìš©]
    end
    
    subgraph "Output Layer"
        Q --> R[res.json ìƒì„±]
        R --> S[SQLite DB ì €ì¥]
        R --> T[API ì‘ë‹µ]
    end
    
    style J fill:#FF6B6B
    style L fill:#FF6B6B  
    style N fill:#FF6B6B
    style K fill:#4ECDC4
    style M fill:#4ECDC4
    style O fill:#4ECDC4
```

## ğŸ¯ í”„ë ˆì„ì›Œí¬ë³„ ì—­í•  ë¶„ë‹´

### PyTorch Transformer ë ˆì´ì–´

#### í•µì‹¬ ì—­í• 
- **ìˆœì„œ íŒ¨í„´ í•™ìŠµ**: ë²ˆí˜¸ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§
- **í™•ë¥  ë¶„í¬ ì˜ˆì¸¡**: ê° ë²ˆí˜¸ì˜ ì„ íƒ í™•ë¥ ì„ ì •êµí•˜ê²Œ ê³„ì‚°
- **ë³µì¡í•œ ìƒê´€ê´€ê³„**: Multi-head Attentionìœ¼ë¡œ ë²ˆí˜¸ê°„ ì—°ê´€ì„± í¬ì°©

#### ì•„í‚¤í…ì²˜ êµ¬ì„±

```python
class LotteryTransformer(nn.Module):
    def __init__(self, 
                 d_model=128,           # ì„ë² ë”© ì°¨ì›
                 n_heads=8,             # Attention Head ìˆ˜
                 n_layers=4,            # Transformer Layer ìˆ˜
                 dim_feedforward=512,   # FFN ì°¨ì›
                 max_seq_len=50):       # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        
        super().__init__()
        
        # ë²ˆí˜¸ ì„ë² ë”© (1-45 ë²ˆí˜¸ â†’ d_model ì°¨ì›)
        self.number_embedding = nn.Embedding(46, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© (ì‹œê°„ ìˆœì„œ ì •ë³´)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # Transformer ì¸ì½”ë” ìŠ¤íƒ
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        
        # ì¶œë ¥ ë ˆì´ì–´ (45ê°œ ë²ˆí˜¸ì— ëŒ€í•œ í™•ë¥ )
        self.output_head = nn.Linear(d_model, 45)
        
    def forward(self, x):
        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        embedded = self.number_embedding(x) * math.sqrt(self.d_model)
        embedded = self.positional_encoding(embedded)
        
        # Transformer ì¸ì½”ë”©
        transformer_out = self.transformer(embedded)
        
        # í™•ë¥  ë¶„í¬ ì˜ˆì¸¡
        logits = self.output_head(transformer_out[:, -1, :])  # ë§ˆì§€ë§‰ í† í° ì‚¬ìš©
        probabilities = F.softmax(logits, dim=-1)
        
        return probabilities
```

#### ì„±ëŠ¥ íŠ¹ì„±
- **ì¶”ë¡  ì‹œê°„**: 80-100ms (GPU ê¸°ì¤€)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 512MB (ëª¨ë¸ + ì¶”ë¡ )
- **ì •í™•ë„**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ +8-10%
- **ê°•ì **: ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬, ì°½ì˜ì  ì¡°í•©

### scikit-learn ì•™ìƒë¸” ë ˆì´ì–´  

#### í•µì‹¬ ì—­í• 
- **í†µê³„ì  íŠ¹ì„± ìµœì í™”**: í™€ì§ë¹„ìœ¨, ACê°’, ëìˆ˜í•© ë“± ë¶„ì„
- **ì•ˆì •ì  ì˜ˆì¸¡**: ê²€ì¦ëœ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ì‹ ë¢°ë„ í™•ë³´
- **í˜¸ê¸°ë³„ íŠ¹í™”**: ê° í˜¸ê¸°ì˜ í†µê³„ì  íŠ¹ì„±ì— ë§ëŠ” ìµœì í™”

#### ì•™ìƒë¸” êµ¬ì„±

```python
class EnhancedLotteryPredictor:
    def __init__(self):
        # 1. Random Forest: íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ ì˜ˆì¸¡
        self.rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        
        # 2. Gradient Boosting: ìˆœì°¨ì  ì˜¤ì°¨ ë³´ì •
        self.gbm_model = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6
        )
        
        # 3. Multi-layer Perceptron: ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ
        self.mlp_model = MLPRegressor(
            hidden_layer_sizes=(100, 50),
            activation='relu',
            solver='adam',
            max_iter=500
        )
        
        # 4. Voting Ensemble: ìµœì¢… í†µí•©
        self.ensemble = VotingRegressor([
            ('rf', self.rf_model),
            ('gbm', self.gbm_model), 
            ('mlp', self.mlp_model)
        ], weights=[0.4, 0.4, 0.2])  # ê°€ì¤‘ì¹˜ ì¡°ì •
        
    def extract_features(self, draw_data):
        """í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        # 1. ë²ˆí˜¸ ë¹ˆë„ íŠ¹ì„±
        number_frequency = self._calculate_frequency(draw_data)
        features.extend(number_frequency)
        
        # 2. í™€ì§ ë¹„ìœ¨ íŠ¹ì„±
        odd_even_ratio = self._calculate_odd_even(draw_data)
        features.append(odd_even_ratio)
        
        # 3. ê³ ì € ë¹„ìœ¨ íŠ¹ì„±  
        high_low_ratio = self._calculate_high_low(draw_data)
        features.append(high_low_ratio)
        
        # 4. ACê°’ (Arithmetic Complexity)
        ac_value = self._calculate_ac_value(draw_data)
        features.append(ac_value)
        
        # 5. ëìˆ˜í•© íŠ¹ì„±
        last_digit_sum = self._calculate_last_digit_sum(draw_data)
        features.append(last_digit_sum)
        
        # 6. ì´í•© íŠ¹ì„±
        total_sum = sum(draw_data)
        features.append(total_sum)
        
        return np.array(features).reshape(1, -1)
```

#### ì„±ëŠ¥ íŠ¹ì„±
- **ì¶”ë¡  ì‹œê°„**: 30-50ms (CPU ê¸°ì¤€)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 256MB (3ê°œ ëª¨ë¸ + ì•™ìƒë¸”)
- **ì •í™•ë„**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ +6-8%
- **ê°•ì **: ë†’ì€ ì•ˆì •ì„±, í•´ì„ ê°€ëŠ¥ì„±

## ğŸ”„ ëª¨ë¸ ì¡°í•© ì „ëµ

### 1. Weighted Voting (ê°€ì¤‘ íˆ¬í‘œ)

```python
class WeightedVotingStrategy:
    def __init__(self):
        # ê° ëª¨ë¸ì˜ ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
        self.weights = {
            'pytorch_transformer': 0.55,    # ì°½ì˜ì„± ì¤‘ì‹¬
            'sklearn_ensemble': 0.45        # ì•ˆì •ì„± ì¤‘ì‹¬  
        }
        
    def combine_predictions(self, pytorch_pred, sklearn_pred):
        """ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"""
        
        # í™•ë¥  ë¶„í¬ ì •ê·œí™”
        pytorch_normalized = self._normalize_probabilities(pytorch_pred)
        sklearn_normalized = self._normalize_probabilities(sklearn_pred)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        combined_prob = (
            self.weights['pytorch_transformer'] * pytorch_normalized +
            self.weights['sklearn_ensemble'] * sklearn_normalized
        )
        
        # ìƒìœ„ 6ê°œ ë²ˆí˜¸ ì„ íƒ
        top_indices = np.argsort(combined_prob)[-6:]
        selected_numbers = [idx + 1 for idx in sorted(top_indices)]
        
        return {
            'numbers': selected_numbers,
            'confidence': float(np.mean(combined_prob[top_indices])),
            'method': 'weighted_voting',
            'weights': self.weights
        }
```

### 2. Stacking (ìŠ¤íƒœí‚¹)

```python
class StackingStrategy:
    def __init__(self):
        # ë©”íƒ€ ëª¨ë¸: ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìµœì¢… ì˜ˆì¸¡
        self.meta_model = LinearRegression()
        self.is_trained = False
        
    def train_meta_model(self, historical_data):
        """ë©”íƒ€ ëª¨ë¸ í›ˆë ¨"""
        X_meta = []  # [pytorch_pred, sklearn_pred]
        y_meta = []  # ì‹¤ì œ ê²°ê³¼
        
        for data in historical_data:
            pytorch_pred = self.pytorch_model.predict(data['input'])
            sklearn_pred = self.sklearn_model.predict(data['input'])
            
            meta_features = np.concatenate([pytorch_pred, sklearn_pred])
            X_meta.append(meta_features)
            y_meta.append(data['actual'])
            
        self.meta_model.fit(X_meta, y_meta)
        self.is_trained = True
        
    def combine_predictions(self, pytorch_pred, sklearn_pred):
        """ë©”íƒ€ ëª¨ë¸ë¡œ ìµœì¢… ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ë©”íƒ€ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        meta_features = np.concatenate([pytorch_pred, sklearn_pred]).reshape(1, -1)
        final_prediction = self.meta_model.predict(meta_features)[0]
        
        return {
            'numbers': self._extract_top_numbers(final_prediction),
            'confidence': self._calculate_confidence(final_prediction),
            'method': 'stacking'
        }
```

### 3. Blending (ë¸”ë Œë”©)

```python
class BlendingStrategy:
    def __init__(self):
        # ë™ì  ê°€ì¤‘ì¹˜: ê° ëª¨ë¸ì˜ ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •
        self.performance_history = {
            'pytorch': deque(maxlen=50),
            'sklearn': deque(maxlen=50)
        }
        
    def update_performance(self, pytorch_score, sklearn_score):
        """ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        self.performance_history['pytorch'].append(pytorch_score)
        self.performance_history['sklearn'].append(sklearn_score)
        
    def calculate_dynamic_weights(self):
        """ìµœê·¼ ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if len(self.performance_history['pytorch']) < 10:
            return {'pytorch': 0.5, 'sklearn': 0.5}  # ê¸°ë³¸ê°’
            
        pytorch_avg = np.mean(self.performance_history['pytorch'])
        sklearn_avg = np.mean(self.performance_history['sklearn'])
        
        total = pytorch_avg + sklearn_avg
        weights = {
            'pytorch': pytorch_avg / total,
            'sklearn': sklearn_avg / total
        }
        
        return weights
        
    def combine_predictions(self, pytorch_pred, sklearn_pred):
        """ë™ì  ê°€ì¤‘ì¹˜ë¡œ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"""
        weights = self.calculate_dynamic_weights()
        
        combined_result = (
            weights['pytorch'] * pytorch_pred +
            weights['sklearn'] * sklearn_pred  
        )
        
        return {
            'numbers': self._select_final_numbers(combined_result),
            'confidence': self._estimate_confidence(combined_result, weights),
            'method': 'dynamic_blending',
            'weights': weights
        }
```

## âš¡ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”

### ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ

```python
class HybridModelManager:
    def __init__(self, max_memory_mb=2048):
        self.max_memory = max_memory_mb * 1024 * 1024  # bytes
        self.memory_allocation = {
            'pytorch_model': 512 * 1024 * 1024,   # 512MB
            'sklearn_models': 256 * 1024 * 1024,  # 256MB  
            'data_cache': 128 * 1024 * 1024,      # 128MB
            'system_buffer': 256 * 1024 * 1024    # 256MB
        }
        
        # ëª¨ë¸ ë¡œë”© ìˆœì„œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤)
        self.loading_order = [
            'sklearn_models',    # ë¨¼ì € ì‘ì€ ëª¨ë¸ë“¤ ë¡œë“œ
            'pytorch_model'      # í° ëª¨ë¸ì€ ë‚˜ì¤‘ì— ë¡œë“œ
        ]
        
    def load_models_sequentially(self):
        """ìˆœì°¨ì  ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”"""
        loaded_models = {}
        
        for model_type in self.loading_order:
            current_memory = self._get_current_memory_usage()
            required_memory = self.memory_allocation[model_type]
            
            if current_memory + required_memory > self.max_memory:
                self._cleanup_cache()  # ìºì‹œ ì •ë¦¬
                
            if model_type == 'sklearn_models':
                loaded_models['sklearn'] = self._load_sklearn_models()
            elif model_type == 'pytorch_model':
                loaded_models['pytorch'] = self._load_pytorch_model()
                
        return loaded_models
        
    def _cleanup_cache(self):
        """ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ìºì‹œ ì •ë¦¬"""
        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()              # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
```

### GPU/CPU í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬

```python
class HybridInferenceEngine:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.pytorch_model = None
        self.sklearn_models = None
        
    async def parallel_inference(self, input_data):
        """ë³‘ë ¬ ì¶”ë¡  ì‹¤í–‰"""
        
        # ì‘ì—… ë¶„ë°°: PyTorchëŠ” GPU, sklearnì€ CPU
        pytorch_task = asyncio.create_task(
            self._pytorch_inference_gpu(input_data)
        )
        
        sklearn_task = asyncio.create_task(
            self._sklearn_inference_cpu(input_data)
        )
        
        # ë³‘ë ¬ ì‹¤í–‰ ëŒ€ê¸°
        pytorch_result, sklearn_result = await asyncio.gather(
            pytorch_task, sklearn_task, return_exceptions=True
        )
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        if isinstance(pytorch_result, Exception):
            logger.error(f"PyTorch ì¶”ë¡  ì‹¤íŒ¨: {pytorch_result}")
            pytorch_result = self._get_fallback_result()
            
        if isinstance(sklearn_result, Exception):
            logger.error(f"sklearn ì¶”ë¡  ì‹¤íŒ¨: {sklearn_result}")
            sklearn_result = self._get_fallback_result()
            
        return pytorch_result, sklearn_result
        
    async def _pytorch_inference_gpu(self, data):
        """GPU ê¸°ë°˜ PyTorch ì¶”ë¡ """
        with torch.cuda.device(0):
            input_tensor = torch.tensor(data).to(self.device)
            with torch.no_grad():
                output = self.pytorch_model(input_tensor)
            return output.cpu().numpy()
            
    async def _sklearn_inference_cpu(self, data):
        """CPU ê¸°ë°˜ sklearn ì¶”ë¡ """
        loop = asyncio.get_event_loop()
        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        result = await loop.run_in_executor(
            None, self.sklearn_models.predict, data
        )
        return result
```

## ğŸ° í˜¸ê¸°ë³„ íŠ¹í™” ì „ëµ

### 1í˜¸ê¸°: ì‹ ì¤‘í•œ ì „ëµê°€

```python
class Machine1Strategy:
    """í”„ë¦¬ë¯¸ì—„ ì„ íƒ ì „ëµ - ê³ ìˆ˜ ë²ˆí˜¸ ê°€ì¤‘, ACê°’ ë³µì¡ë„ ì„ í˜¸"""
    
    def __init__(self):
        self.strategy_weights = {
            'high_frequency_numbers': 0.35,    # ê³ ë¹ˆë„ ë²ˆí˜¸ ê°€ì¤‘ì¹˜ +15%
            'ac_complexity': 0.25,             # ACê°’ ë³µì¡ë„ ê°€ì¤‘ì¹˜ +10%
            'balanced_distribution': 0.20,      # ê· í˜• ë¶„í¬
            'conservative_range': 0.20          # ë³´ìˆ˜ì  ë²”ìœ„ ì„ íƒ
        }
        
    def apply_strategy(self, base_prediction):
        """1í˜¸ê¸° ì „ëµ ì ìš©"""
        
        # ê³ ìˆ˜ ë²ˆí˜¸ (ìƒìœ„ 30% ë¹ˆë„) ê°€ì¤‘ì¹˜ ì¦ê°€
        frequent_numbers = self._get_frequent_numbers(top_percent=30)
        for num in frequent_numbers:
            if num in base_prediction:
                base_prediction[num] *= 1.15
                
        # ACê°’ ë³µì¡ë„ ì„ í˜¸ (ACê°’ > 15)
        if self._calculate_ac_value(base_prediction) < 15:
            base_prediction = self._increase_ac_complexity(base_prediction)
            
        # ë³´ìˆ˜ì  ì´í•© ë²”ìœ„ (120-180)
        current_sum = sum(base_prediction)
        if current_sum < 120 or current_sum > 180:
            base_prediction = self._adjust_to_conservative_range(base_prediction)
            
        return base_prediction
```

### 2í˜¸ê¸°: ì™„ë²½ì£¼ì˜ ì¡°ìœ¨ì‚¬

```python
class Machine2Strategy:
    """ì™„ë²½í•œ ì¡°í™” ì „ëµ - ê· í˜• ìµœì í™”, ëìˆ˜í•© ì¤‘ì """
    
    def __init__(self):
        self.strategy_weights = {
            'balance_optimization': 0.40,       # ê· í˜• ì ìˆ˜ ê°€ì¤‘ì¹˜ +20%
            'last_digit_harmony': 0.30,         # ëìˆ˜í•© ìµœì í™” +20%
            'even_distribution': 0.20,          # ê³ ë¥¸ ë¶„í¬
            'golden_ratio': 0.10                # í™©ê¸ˆë¹„ ì ìš©
        }
        
    def apply_strategy(self, base_prediction):
        """2í˜¸ê¸° ì „ëµ ì ìš©"""
        
        # í™€ì§ ê· í˜• ìµœì í™” (3:3 ë˜ëŠ” 4:2)
        odd_count = sum(1 for num in base_prediction if num % 2 == 1)
        if odd_count < 2 or odd_count > 4:
            base_prediction = self._balance_odd_even(base_prediction)
            
        # ëìˆ˜í•© ìµœì í™” (ëìˆ˜í•© 15-25)
        last_digit_sum = sum(num % 10 for num in base_prediction)
        if last_digit_sum < 15 or last_digit_sum > 25:
            base_prediction = self._optimize_last_digit_sum(base_prediction)
            
        # êµ¬ê°„ë³„ ê· ë“± ë¶„í¬ (1-15, 16-30, 31-45 ê° 2ê°œì”©)
        distribution = self._check_section_distribution(base_prediction)
        if not self._is_well_distributed(distribution):
            base_prediction = self._balance_sections(base_prediction)
            
        return base_prediction
```

### 3í˜¸ê¸°: ì°½ì¡°ì  ë„ì „ì

```python
class Machine3Strategy:
    """ì°½ì¡°ì  í˜ì‹  ì „ëµ - í™€ìˆ˜ ì„ í˜¸, ë‹¤ì–‘ì„± ì¶”êµ¬"""
    
    def __init__(self):
        self.strategy_weights = {
            'odd_number_preference': 0.32,      # í™€ìˆ˜ ê°€ì¤‘ì¹˜ +12%
            'diversity_bonus': 0.28,            # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ +8%
            'creative_combinations': 0.25,      # ì°½ì˜ì  ì¡°í•©
            'challenge_patterns': 0.15          # ë„ì „ì  íŒ¨í„´
        }
        
    def apply_strategy(self, base_prediction):
        """3í˜¸ê¸° ì „ëµ ì ìš©"""
        
        # í™€ìˆ˜ ì„ í˜¸ (4-5ê°œ í™€ìˆ˜)
        odd_numbers = [num for num in base_prediction if num % 2 == 1]
        if len(odd_numbers) < 4:
            base_prediction = self._increase_odd_numbers(base_prediction)
            
        # ë‹¤ì–‘ì„± ì¶”êµ¬ (ì—°ì† ë²ˆí˜¸ ìµœì†Œí™”)
        consecutive_pairs = self._count_consecutive_pairs(base_prediction)
        if consecutive_pairs > 1:
            base_prediction = self._reduce_consecutive_numbers(base_prediction)
            
        # ì°½ì˜ì  íŒ¨í„´ (í”¼ë³´ë‚˜ì¹˜, ì†Œìˆ˜ ë“± íŠ¹ìˆ˜ ìˆ˜ì—´ í¬í•¨)
        special_numbers = self._get_special_pattern_numbers()
        intersection = set(base_prediction) & set(special_numbers)
        if len(intersection) < 2:
            base_prediction = self._add_special_numbers(base_prediction)
            
        return base_prediction
```

## ğŸ“Š ì„±ëŠ¥ ë³‘ëª©ì§€ì  ë¶„ì„

### 1. ëª¨ë¸ ë¡œë”© ë³‘ëª©

```python
# ë¬¸ì œ: ìˆœì°¨ì  ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì§€ì—°
def load_models_sequential():
    pytorch_model = load_pytorch_model()    # 25ì´ˆ
    sklearn_model = load_sklearn_model()    # 8ì´ˆ  
    return pytorch_model, sklearn_model     # ì´ 33ì´ˆ

# í•´ê²°: ë¹„ë™ê¸° ë³‘ë ¬ ë¡œë”©
async def load_models_parallel():
    pytorch_task = asyncio.create_task(load_pytorch_model_async())
    sklearn_task = asyncio.create_task(load_sklearn_model_async()) 
    
    pytorch_model, sklearn_model = await asyncio.gather(
        pytorch_task, sklearn_task
    )
    return pytorch_model, sklearn_model     # ì´ 25ì´ˆ (30% ê°œì„ )
```

### 2. ì¶”ë¡  ë³‘ëª©

```python
# ë¬¸ì œ: GPU ë©”ëª¨ë¦¬ ì „ì†¡ ì˜¤ë²„í—¤ë“œ
def inefficient_inference(data_batch):
    results = []
    for data in data_batch:
        gpu_data = torch.tensor(data).cuda()    # ë§¤ë²ˆ GPU ì „ì†¡
        result = model(gpu_data)
        results.append(result.cpu())            # ë§¤ë²ˆ CPU ì „ì†¡
    return results

# í•´ê²°: ë°°ì¹˜ ì²˜ë¦¬ + ë©”ëª¨ë¦¬ ìµœì í™”
def efficient_inference(data_batch):
    gpu_batch = torch.stack([torch.tensor(d) for d in data_batch]).cuda()
    with torch.no_grad():
        results = model(gpu_batch)
    return results.cpu()    # í•œë²ˆì— ì „ì†¡ (70% ê°œì„ )
```

### 3. ê²°ê³¼ ë³‘í•© ë³‘ëª©

```python
# ë¬¸ì œ: JSON ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ
def slow_result_aggregation(pytorch_results, sklearn_results):
    combined = []
    for pt_res, sk_res in zip(pytorch_results, sklearn_results):
        result = {
            'pytorch_prediction': pt_res.tolist(),
            'sklearn_prediction': sk_res.tolist(),
            'ensemble_result': combine_predictions(pt_res, sk_res),
            'metadata': generate_metadata(pt_res, sk_res)
        }
        combined.append(json.dumps(result))  # ë§¤ë²ˆ ì§ë ¬í™”
    return combined

# í•´ê²°: ë°°ì¹˜ ì§ë ¬í™” + ì••ì¶•
def fast_result_aggregation(pytorch_results, sklearn_results):
    # ë¨¼ì € ëª¨ë“  ê²°ê³¼ë¥¼ ê²°í•©
    combined_data = []
    for pt_res, sk_res in zip(pytorch_results, sklearn_results):
        combined_data.append({
            'pytorch_prediction': pt_res,
            'sklearn_prediction': sk_res,
            'ensemble_result': combine_predictions(pt_res, sk_res)
        })
    
    # í•œë²ˆì— ì§ë ¬í™” + ì••ì¶•
    compressed_result = compress_json(combined_data)
    return compressed_result  # 60% ê°œì„ 
```

## ğŸ”§ ìµœì í™” ì‹¤í–‰ ê³„íš

### Phase 1: ê¸°ë³¸ ìµœì í™” (ì™„ë£Œ)
- âœ… ìˆœì°¨ì  ëª¨ë¸ ë¡œë”© êµ¬í˜„
- âœ… ê¸°ë³¸ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
- âœ… í˜¸ê¸°ë³„ ì „ëµ ì°¨ë³„í™”

### Phase 2: ì„±ëŠ¥ ìµœì í™” (ì§„í–‰ ì¤‘)
- ğŸ”„ ë¹„ë™ê¸° ë³‘ë ¬ ì¶”ë¡  êµ¬í˜„
- ğŸ”„ GPU/CPU í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬
- ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

### Phase 3: ê³ ê¸‰ ìµœì í™” (ê³„íš)
- ğŸ¯ TensorRT/ONNX ëª¨ë¸ ë³€í™˜
- ğŸ¯ ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì • ì‹œìŠ¤í…œ
- ğŸ¯ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

ì´ í•˜ì´ë¸Œë¦¬ë“œ ML ì•„í‚¤í…ì²˜ëŠ” ê° í”„ë ˆì„ì›Œí¬ì˜ ê°•ì ì„ ìµœëŒ€í•œ í™œìš©í•˜ë©´ì„œë„, ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ ë™ì‹œì— í™•ë³´í•œ ì„¤ê³„ì…ë‹ˆë‹¤. ğŸš€